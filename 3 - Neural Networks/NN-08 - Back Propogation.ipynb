{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks - Back Propogation\n",
    "\n",
    "- <font color=\"red\">The following section is, I think, the most complicated thing till now, so</font>\n",
    "- <font color=\"red\">We take a second to explain the general idea of what we're going to do:</font>\n",
    "\n",
    "We've already described __*<font color=\"blue\" size=\"3em\">forward propagation</font>*__\n",
    "- This is the algorithm which takes your neural network and the initial input into that network \n",
    "  and <font color=\"blue\">pushes the input through the network</font>\n",
    "  - It leads to the generation of an output hypothesis, which may be a single real number, but can also be a vector\n",
    "\n",
    "We're now going to describe __*<font color=\"blue\" size=\"3em\">back propagation</font>*__\n",
    "- Back propagation basically takes the output you got from your network, \n",
    "  <br>compares it to the expected real value $(y)$ and calculates how wrong the network was \n",
    "  <br>(i.e. how wrong the parameters were)\n",
    "- It then, using the error you've just calculated, \n",
    "  <font color=\"blue\">back-calculates the error associated with each unit from the preceding layer \n",
    "  (i.e. layer L - 1)</font>\n",
    "- This goes on until you reach the input layer (where obviously there is no error, as the activation is the input)\n",
    "- These \"error\" measurements for each unit can be then used to calculate the __*partial derivatives*__\n",
    "  - <font color=\"blue\">Partial derivatives are then used by *gradient descent* to minimize the cost function</font>\n",
    "- We use the partial derivatives with gradient descent to try minimize the cost function \n",
    "  and update all the $Ɵ$ values\n",
    "- This repeats until gradient descent reports convergence\n",
    "\n",
    "<u>A few things to remember</u>:\n",
    "- There is a <font color=\"blue\">Ɵ matrix for each layer in the network</font>\n",
    "  - This has each node in layer $l$ as one dimension (rows) and each node in $l+1$ as the other dimension (columns)\n",
    "- Similarly, there is going to be a <font color=\"blue\">Δ matrix for each layer</font>\n",
    "  - This has each node as one dimension and each training data example as the other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Comutation\n",
    "\n",
    "- <font color=\"blue\">*Back propagation* is an algorithm used for calculating partial derivatives!</font>\n",
    "  <br>later used by *gradient descent* for minimizing the cost function\n",
    "\n",
    "\n",
    "- Neural network cost function:\n",
    "<img src=\"images/neural-network-cost-function.png\">\n",
    "\n",
    "- ### <font color=\"blue\">We want to find parameters $Ɵ$ which minimize $J(Ɵ)$</font>\n",
    "\n",
    "- To do so we can use one of\n",
    "  - <font color=\"blue\">Gradient descent</font>\n",
    "  - Advanced optimization algorithms (e.g., <font color=\"blue\">Conjugate gradient, BFGS, L-BFGS</font>)\n",
    "\n",
    "\n",
    "- To minimize a cost function we just <font color=\"blue\">write code which computes</font> the following\n",
    "  - <font color=\"blue\">$J(Ɵ)$</font>\n",
    "    - i.e. the cost function itself! (use the formula above to calculate this value)\n",
    "  - <font color=\"blue\">Partial derivative terms</font>\n",
    "    - $Ɵ$ is indexed in 3-dimensions because each layer has a $Ɵ$ matrix associated with it!\n",
    "      - We want to calculate the partial derivative $Ɵ$ with respect to a single parameter\n",
    "      <img src=\"images/D-J-theta.png\">\n",
    "      - The partial derivative term we calculate above is a REAL number (not a vector or a matrix)\n",
    "\n",
    "- ### <font color=\"blue\">We next focus on how to compute these partial derivative terms</font>\n",
    "  - We start by talking about the case where we have only one training example\n",
    "  - The first thing we do is applying <font color=\"blue\">forward propogation</font> to get $H_Ɵ(x)$\n",
    "  <img src=\"images/NN-4-layers-Forward-Propogation.png\">\n",
    "  - The forward propagation algorithm operates as follows (vectorized implementation)\n",
    "    - **Layer 1** (Input)\n",
    "      - $a^1 = x$\n",
    "    - **Layer 2**\n",
    "      - $z^2 = Ɵ^1a^1$    {add $a_0^1$}\n",
    "      - $a^2 = g(z^2)$\n",
    "    - **Layer 3**\n",
    "      - $z^3 = Ɵ^2a^2$    {add $a_0^2$}\n",
    "      - $a^3 = g(z^3)$\n",
    "    - **Layer 4** (Output)\n",
    "      - $z^4 = Ɵ^3a^3$    {add $a_0^3$}\n",
    "      - $a^4 = H_Ɵ(x) = g(z^4)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, in order to compute the derivatives, we're going to use\n",
    "## <font color=\"blue\">Backpropogation Algorithm</font>\n",
    "\n",
    "- For each node at layer $l$ - calculate <font color=\"blue\">($δ_j^l$)</font> - \n",
    "  this is the <font color=\"blue\">error of node $j$ in layer $l$</font>\n",
    "  - $δ_j^l$ captures the node activation ($a_j^l$) error compared to the \"real\" value\n",
    "  - We start from the end: <font color=\"blue\">$δ_j^4 = a_j^4 - y_j$</font>\n",
    "  - Instead of focussing on each node, let's think about this as a vectorized problem\n",
    "    - <font color=\"blue\">$δ^4 = a^4 - y$</font>\n",
    "      - So here $δ^4$ is the vector of errors for the 4<sup>th</sup> layer\n",
    "      - $a^4$ is the vector of activation values for the 4<sup>th</sup> layer\n",
    "  - With δ4 calculated, we can determine the error terms for the other layers as follows:\n",
    "    <img src=\"images/delta-calc-layers-3-and-2.png\">\n",
    "    - Taking a second to break this down\n",
    "      - $Ɵ^3$ is the vector of parameters for the 3 -> 4 layer mapping\n",
    "      - $δ^4$ is (as calculated) the error vector for the 4<sup>th</sup> layer\n",
    "      - $g'(z^3)$ is the derivative of the activation function $g$ \n",
    "        evaluated by the input values given by $z^3$\n",
    "        - When you calculate this derivative you get $g'(z^3) = a^3$ .\\* $(1 - a^3)$\n",
    "      - So, <font color=\"blue\">$δ^3 = (Ɵ^3)^T δ^4$ .\\* $(a^3$ .\\* $(1 - a^3))$</font>\n",
    "        - <font color=\"red\"><b>.\\*</b></font> is the element wise multiplication between the two vectors\n",
    "        - Why element wise? Because this is essentially an extension of individual values in a \n",
    "          vectorized implementation\n",
    "  - There's no $δ^1$ term because that's the input!\n",
    " \n",
    "### Why do we do all this?\n",
    " - We do all this to get all the δ terms, and we want the δ terms because through a very complicated derivation \n",
    "   <br>you can use δ to get the partial derivative of Ɵ with respect to individual parameters \n",
    "   <br>(if you ignore regularization, or regularization is 0, which we deal with later)\n",
    "   <img src=\"images/D-J-theta-and-delta.png\">\n",
    "\n",
    "### Putting it all together to get the partial derivatives!\n",
    "- Training set of m examples\n",
    "  <img src=\"images/training-set.png\">\n",
    "- Init the delta values\n",
    "  - Will be used as accumulators for computing the partial derivatives\n",
    "  <img src=\"images/delta-i-j-init.png\">\n",
    "- Loop through the training set:\n",
    "  - i.e. for each example in the training set - dealing with each example as $(x,y)$\n",
    "  - Set $a^1$ (activation of input layer) = $x_i$\n",
    "  - for $i = 1$ to $m$\n",
    "    - Perform <font color=\"blue\">forward propagation</font> to compute $a^l$ for each layer (l = 1,2, ... L)\n",
    "    - Then, use the output label for the specific example we're looking at to calculate \n",
    "      $δ^L$ where $δ^L = a^L - y^i$\n",
    "      - So we initially calculate the delta value for the output layer\n",
    "      - Then, using <font color=\"blue\">back propagation</font> we move back through the network \n",
    "        from layer $L-1$ down to layer 1\n",
    "      - Finally, use Δ to accumulate the partial derivative terms\n",
    "        <img src=\"images/delta-i-j-calc.png\">\n",
    "      - You can vectorize the Δ expression too, as\n",
    "        <img src=\"images/delta-i-j-calc-vectorized.png\">\n",
    "  - Finally, after executing the body of the loop, exit the for loop and compute\n",
    "    - (when $j = 0$ we have no regularization term)\n",
    "    <img src=\"images/D-i-j-calc.png\">\n",
    "- At the end of ALL this\n",
    "  - You've calculated all the D terms above using Δ\n",
    "    - each D term above is a real number!\n",
    "  - It can be shown that each D is equal to the following\n",
    "  <img src=\"images/bp-final-step.png\">\n",
    "\n",
    "### Finally, we have calculated the partial derivative for each parameter\n",
    "- We can then use these in gradient descent or one of the advanced optimization algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back propagation intuition\n",
    "\n",
    "\n",
    "### Forward propagation with pictures!\n",
    "\n",
    "  <img src=\"images/BP-intuition-1.png\">\n",
    "- With out input data present we use **forward propagation**\n",
    "  <img src=\"images/BP-intuition-2.png\">\n",
    "- The sigmoid function applied to the z values gives the activation values\n",
    "  - Below we show exactly how the z value is calculated for an example\n",
    "  <img src=\"images/BP-intuition-3.png\">\n",
    "\n",
    "### Back propagation\n",
    "\n",
    "- Back propagation is doing something very similar to forward propagation, but backwards\n",
    "  - Let's look at the cost function again...\n",
    "    - Below we have the cost function if there is a single output\n",
    "    <img src=\"images/BP-intuition-4.png\">\n",
    "    - This function cycles over each example, so the cost for one example really boils down to this\n",
    "    <img src=\"images/BP-intuition-5.png\">\n",
    "    - Which, we can think of as a sigmoidal version of the squared difference\n",
    "      - So, basically saying, \"how well is the network doing on example i \"?\n",
    "    - We can think about a δ term on a unit as the \"error\" of cost for the activation value associated with a unit\n",
    "      - More formally, δ is\n",
    "        <img src=\"images/BP-intuition-6.png\">\n",
    "        - Where cost is as defined above\n",
    "        - Cost function is a function of y value and the hypothesis function\n",
    "    - So, for the output layer, back propagation sets the δ value as [a - y]\n",
    "      - Difference between activation and actual value\n",
    "    - We then propagate these values backwards:\n",
    "      <img src=\"images/BP-intuition-7.png\">\n",
    "    - Looking at another example to see how we actually calculate the delta value\n",
    "      <img src=\"images/BP-intuition-8.png\">\n",
    "    \n",
    "- So, in effect\n",
    "  - Back propagation calculates the δ, and those δ values are the weighted sum of the \n",
    "    next layer's delta values, weighted by the parameter associated with the links"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
