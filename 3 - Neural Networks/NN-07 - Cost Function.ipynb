{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks - Cost Function\n",
    "Focus on application of NNs for classification problems\n",
    "- Here's the set up\n",
    "  - Training set is {$(x_1, y_1), (x_2, y_2), (x_3, y_3) ... (x_n, y_m)$}\n",
    "  - $L$ = number of layers in the network\n",
    "    - In our example below $L = 4$\n",
    "  - $s_l$ = number of units (not counting bias unit) in layer $l$\n",
    "    - So here: $l = 4, s_1 = 3, s_2 = 5, s_3 = 5, s_4 = 4$\n",
    "  - $k$ is number of units in output layer\n",
    "    - So here: $k = 4$\n",
    "  <img src=\"images/NN-4-layers.png\">\n",
    "  - <font color=\"blue\">Binary classification</font>\n",
    "    - 1 output (0 or 1)\n",
    "    - $k = 1$\n",
    "    - $s_L = 1$\n",
    "  <p>\n",
    "  - <font color=\"blue\">Multi-class classification</font>\n",
    "    - $k$ distinct classifications\n",
    "    - Typically $k$ is greater than or equal to 3\n",
    "      - If only two just go for binary\n",
    "    - $s_L = k$\n",
    "    - So $y$ is a $k$-dimensional vector of real numbers\n",
    "    <img src=\"images/NN-4-layers-output.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function for neural networks\n",
    "- The (regularized) logistic regression cost function is as follows:\n",
    "<img src=\"images/logistic-regression-cost-function.png\">\n",
    "<p>\n",
    "- For neural networks our cost function is a generalization of this equation above, \n",
    "  - So instead of one output we generate k outputs\n",
    "<img src=\"images/neural-network-cost-function.png\">\n",
    "<p>\n",
    "- Our <font color=\"blue\">cost function now outputs a $k$-dimensional vector</font>\n",
    "  - <font color=\"blue\">$h_Ɵ(x)$ is a $k$-dimensional vector</font>, \n",
    "    so $h_Ɵ(x)_i$ refers to the i<sup>th</sup> value in that vector\n",
    "\n",
    "<p>\n",
    "- Cost function $J(Ɵ)$ is\n",
    "  - $-1/m$ times a sum of a similar term to which we had for logic regression\n",
    "  - But now this is also a sum from $k = 1$ through to $K$ ($K$ is number of output nodes)\n",
    "    - Sum over the $k$ output units - i.e. for each of the possible classes\n",
    "    - E.g., if 4 output units then the sum is $k = 1$ to 4\n",
    "  - NB: We don't sum over the bias terms (hence starting at 1 for the summation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks really complicated, but it's not so difficult\n",
    "<font color=\"red\" size=\"4em\">Lets take a second to try and understand this!</font>\n",
    "\n",
    "- There are basically two halves to the neural network logistic regression cost function\n",
    "  - **First half**\n",
    "    <img src=\"images/logistic-regression-cost-function - first half.png\">\n",
    "    - This is just saying:\n",
    "      - For <font color=\"blue\">each training data example</font> (i.e. 1 to m - the first summation)\n",
    "         - Sum <font color=\"blue\">over all output units</font>\n",
    "    - This is an <font color=\"blue\">average sum of logistic regression</font>\n",
    "\n",
    "<p>\n",
    "  - **Second half**\n",
    "    <img src=\"images/logistic-regression-cost-function - second half.png\">\n",
    "    - This is a massive regularization summation term, which I'm not going to walk through, \n",
    "      <br>but it's a fairly straightforward triple nested summation\n",
    "      <br>Intuition: It penalize all the $Ɵ$<sub>ij</sub> weights for all layers\n",
    "    - This is also called a *<font color=\"blue\">weight decay term</font>*\n",
    "    - As before, the $\\lambda$ value determines the important of the two halves\n",
    " \n",
    " <p>\n",
    " <font color=\"red\" size=\"3em\">So, we have a cost function, but how do we minimize this bad boy?!</font>\n",
    " ### Next: Back Propogation ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
