{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistics Regression - Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The problem of overfitting\n",
    "- What is overfitting?\n",
    "- What is regularization and how does it help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting with linear regression\n",
    "- Using our house pricing example again\n",
    "- Fit a linear function to the data - ** not a great model **\n",
    "- This is **<font color=\"#E30000\">underfitting</font>** - \n",
    "  also known as **<font color=\"#E30000\">high bias</font>**\n",
    "  - Bias - if we're fitting a straight line to the data we have a strong preconception \n",
    "    that there should be a linear fit\n",
    "  - In this case, this is not correct, but a straight line can't help being straight!\n",
    "  <img src=\"images/underfitting.png\">\n",
    "- Fit a quadratic function\n",
    "  - Works well **<font color=\"#1C3387\">(\"just right\")</font>**\n",
    "  - But - goes down when size of house get bigger (not shown in the drawing)\n",
    "    - maybe should use **kubic** polinomial...\n",
    "  <img src=\"images/just right.png\">\n",
    "- Fit a 4th order polynomial\n",
    "  - Now curve fit's through all five examples\n",
    "  - Seems to do a good job fitting the **training set**\n",
    "  - But, despite fitting the data we've provided very well, this is actually not such a good model\n",
    "    - It doesn't generalize well!\n",
    "    - This is **<font color=\"#E30000\">overfitting</font>** - \n",
    "      also known as **<font color=\"#E30000\">high variance</font>**\n",
    "  <img src=\"images/overfitting.png\">      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To recap\n",
    "- if we have too many features then the learned hypothesis may give a cost function of exactly zero\n",
    "  - But this tries too hard to fit the training set\n",
    "  - Fails to provide a general solution - **<font color=\"#E30000\">unable to generalize</font>** (apply to new examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting with logistic regression\n",
    "- Same thing can happen to logistic regression\n",
    "  - Sigmoid function is an underfit\n",
    "  - But a high order polynomial gives and overfitting (high variance hypothesis)\n",
    "  <img src=\"images/overfitting with logistics regression.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Addressing overfitting\n",
    "- Later we'll look at identifying when overfitting and underfitting is occurring\n",
    "- Plotting hypothesis is one way to decide (that it looks \"too curvy\"), but doesn't always work\n",
    "- Often have lots of a features - harder to plot the data and visualize to decide \n",
    "  what features to keep and which to drop\n",
    "- If you have lots of features and little data - overfitting can be a problem\n",
    "\n",
    "### How do we deal with this?\n",
    "- **Reduce number of features**\n",
    "  - Manually select which features to keep\n",
    "  - Use model selection algorithms (will not be covered here)\n",
    "  - But, in reducing the number of features we lose some information\n",
    "- **<font color=\"#1C3387\" size=\"4em\">Regularization</font>**\n",
    "  - Keep all features, but **<font color=\"#1C3387\" size=\"4em\">reduce magnitude of parameters $θ$</font>**\n",
    "  - Works well when we have a lot of features, each of which contributes a bit to predicting $y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function optimization for regularization\n",
    "- Penalize and make some of the $θ$ parameters really small\n",
    "- e.g. here $θ_3$ and $θ_4$\n",
    "  - modify our cost function to help penalize $θ_3$ and $θ_4$\n",
    "<img src=\"images/regularization - 1.png\">\n",
    "- So here we end up with $θ_3$ and $θ_4$ being close to zero\n",
    "  - So we're basically left with a quadratic function\n",
    "<img src=\"images/regularization - 2.png\">\n",
    "- In this example, we penalized two of the parameter values\n",
    "- More generally, regularization is as follows\n",
    "<img src=\"images/regularization - 3.png\">\n",
    "  - By convention you don't penalize $θ_0$ - minimization is from $θ_1$ onwards\n",
    "  - **<font color=\"#1C3387\" size=\"3em\">$λ$</font>** \n",
    "    is the **<font color=\"#1C3387\" size=\"3em\">regularization parameter</font>**\n",
    "    - Controls a trade off between our two goals\n",
    "      - Want to fit the training set well\n",
    "      - Want to keep parameters small\n",
    "  \n",
    "- ### Small values for parameters corresponds to a simpler hypothesis (smoother curve)\n",
    "  - You effectively get rid of some of the terms\n",
    "  - A simpler hypothesis is less prone to overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized linear regression\n",
    "<img src=\"images/regularization - 4.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
