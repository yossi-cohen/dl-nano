{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent Intuition\n",
    "\n",
    "<img src=\"images/gd-partial-derivative.png\">\n",
    "\n",
    "- We'll return to a simpler function where we minimize only one parameter\n",
    "  - minimize $J(\\theta_1)$ *(where $\\theta_1$ is a real number)*\n",
    "\n",
    "### Assume we only have one parameter $\\theta_1$ $(\\theta_0 == 0)$\n",
    "\n",
    "- Derivative says: \n",
    "  - Take the __*tangent*__ at a point and look at the <u>slope of the *tangent* line</u>\n",
    "  \n",
    "  <br>\n",
    "  \n",
    "  - Moving towards the minimum (<u>down</u>) we get a negative derivative,\n",
    "    - $\\alpha$ is always positive, so will update $\\theta_1$ to a bigger value \n",
    "      and $J(\\theta_1)$ will <u>decrease</u>\n",
    "    <img src=\"images/gradient descent  intuition - 2-1.png\">\n",
    "    \n",
    "  <br>\n",
    "  \n",
    "  - Similarly, Moving <u>up</u> a slope we get a positive derivative,\n",
    "    <br>$\\theta_1$ will be updated to a smaller value and $J(\\theta_1)$ will <u>also decrease</u>.\n",
    "    <img src=\"images/gradient descent  intuition - 2-2.png\">\n",
    "    \n",
    "  <br>\n",
    "  \n",
    "  - When you get to a local minimum\n",
    "    - Gradient of tangent (derivative) is $0$\n",
    "    - So $\\theta_1 = \\theta_1 - 0$ and $\\theta_1$ remains the same *(convergence)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Learning Rate $\\alpha$\n",
    "- Alpha term $(\\alpha)$\n",
    "  - What happens if $\\alpha$ is too small or too large?\n",
    "    - Too small  ->  take baby steps (takes long time to converge)\n",
    "    - Too large  ->  can overshoot the minimum and fail to converge\n",
    "- As you approach the global minimum the derivative term gets smaller, \n",
    "  <br>so the update gets smaller, even when $\\alpha$ is fixed\n",
    "  - Meaning as the algorithm runs, we take smaller steps as we approach the minimum\n",
    "  - So no need to change $\\alpha$ over time\n",
    "<img src=\"images/gradient descent  intuition - 3.png\">\n",
    "\n",
    "## Question\n",
    "\n",
    "<img src=\"images/gradient descent  intuition - 4 - the learning rate alpha.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
