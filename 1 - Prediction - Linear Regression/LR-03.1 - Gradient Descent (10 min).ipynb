{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Gradient Descent - Minimizing the cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap:\n",
    "<img src=\"images/cost function and goal with 2 parameters.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Gradient Descent*\n",
    "- Applies to more general functions: $J(θ_0, θ_1, ..., θ_n)$\n",
    "- Used all over machine learning for *minimization*\n",
    "\n",
    "### How does it work?\n",
    "- Start with an initial random guess for $(θ_0, θ_1)$\n",
    "- Keeping changing $(θ_0, θ_1)$ a little bit to try and reduce $J(θ_0, θ_1)$\n",
    "- Do so until you converge to a local minimum\n",
    "  - Has an interesting property (where you start can determine which minimum you end up)\n",
    "<img src=\"images/gradient-descent.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convex functions\n",
    "- The *squared error* cost function is a convex function - always has a single minimum (bowl shape),\n",
    "  <br>so gradient descent will always converge to global optima.\n",
    "<img src=\"images/J-3D.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A more formal definition\n",
    "\n",
    "<img src=\"images/gradient descent algorithm.png\">\n",
    "\n",
    "- <b><font color=\"blue\">What does this all mean?</font></b>\n",
    "  - Update $θ_j$ by setting it to $(θ_j - α)$ times the partial derivative of the cost function \n",
    "    with respect to $θ_j$\n",
    "    - Do this **simultaneously** for both $θ_0$ and $θ_1$\n",
    "  - **<font color=\"blue\" size=\"4em\">$α$</font>** is a number called the *__<font color=\"blue\">Learning Rate</font>__* (controls how big a step to take)\n",
    "    - If $α$ is big we get an aggressive gradient descent *(may fail to converge)*\n",
    "    - If $α$ is small it takes tiny steps *(can take a long time to converge)*\n",
    "  - Derivative term: not going to talk about it now, derive it later...\n",
    "\n",
    "<p/>\n",
    "### <font color=\"red\">Note!</font>\n",
    "At each step - calculating the derivative\n",
    "<img src=\"images/gd-partial-derivative.png\" />\n",
    "involves calculating $J(θ)$ over all examples\n",
    "<img src=\"images/cost function with 2 parameters.png\" />\n",
    "This is very slow!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
