{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# <font color=\"bordo\">Programming Exercise 1 - Logistic Regression</font>\n",
    "<p>\n",
    "In the first part of the exercise, you will build a logistic regression model to\n",
    "predict whether a student gets admitted into a university.\n",
    "<p>\n",
    "Suppose that you are the administrator of a university department and\n",
    "you want to determine each applicant’s chance of admission \n",
    "<br>based on their results on two exams. \n",
    "<p>\n",
    "You have historical data from previous applicants\n",
    "that you can use as a training set for logistic regression. \n",
    "<br>\n",
    "For each training example, you have the applicant’s scores on two exams and the admissions decision.\n",
    "<br>\n",
    "Your task is to build a classification model that estimates an applicant’s\n",
    "probability of admission based the scores from those two exams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "%matplotlib inline\n",
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loaddata(file, delimeter):\n",
    "    data = np.loadtxt(file, delimiter=delimeter)\n",
    "    print('Dimensions: ',data.shape)\n",
    "    print(data[1:6,:]) # print data samples\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotData(data, label_x, label_y, label_pos, label_neg, axes=None):\n",
    "    # Get indexes for class 0 and class 1\n",
    "    neg = data[:,2] == 0\n",
    "    pos = data[:,2] == 1\n",
    "    \n",
    "    # If no specific axes object has been passed, get the current axes.\n",
    "    if axes == None:\n",
    "        axes = plt.gca()\n",
    "    axes.scatter(data[pos][:,0], data[pos][:,1], marker='+', c='k', s=60, linewidth=2, label=label_pos)\n",
    "    axes.scatter(data[neg][:,0], data[neg][:,1], c='y', s=60, label=label_neg)\n",
    "    axes.set_xlabel(label_x)\n",
    "    axes.set_ylabel(label_y)\n",
    "    axes.legend(frameon= True, fancybox = True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "<a id='loading_data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Dimensions: ', (100, 3))\n",
      "[[ 30.28671077  43.89499752   0.        ]\n",
      " [ 35.84740877  72.90219803   0.        ]\n",
      " [ 60.18259939  86.3085521    1.        ]\n",
      " [ 79.03273605  75.34437644   1.        ]\n",
      " [ 45.08327748  56.31637178   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "data = loaddata('data/ex2data1.txt', ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.c_[np.ones((data.shape[0],1)), data[:,0:2]]\n",
    "y = np.c_[data[:,2]]\n",
    "\n",
    "# np.c_ Translates slice objects to concatenation along the second axis.\n",
    "# Example: np.c_[np.array([[1,2,3]]), 0, 0, np.array([[4,5,6]])]\n",
    "# >>> array([[1, 2, 3, 0, 0, 4, 5, 6]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the data\n",
    "<a id='plotting_data'></a>\n",
    "<p>\n",
    "Before starting to implement any learning algorithm, it is always good to\n",
    "visualize the data if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plotData(data, 'Exam 1 score', 'Exam 2 score', 'Admitted', 'Not admitted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Hypothesis\n",
    "<a id='hypothesis'></a>\n",
    "<p>\n",
    "Logistic Regression hypothesis is defined as:\n",
    "<p>\n",
    "<font size=\"4em\">$h_{\\theta}(x) = g(\\theta^{T}x)$</font>\n",
    "<p>\n",
    "where function g is the sigmoid function. The sigmoid function is defined as:\n",
    "<p>\n",
    "<font size=\"4em\">$g(z)=\\frac{1}{1+e^{−z}}$</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return(1 / (1 + np.exp(-z)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function\n",
    "<a id='cost_function'></a>\n",
    "<p>\n",
    "Recall that the cost function in logistic regression is:\n",
    "<p>\n",
    "<font size=\"3em\">$J(\\theta) = \\frac{1}{m}\\sum_{i=1}^{m}\\big[-y^{(i)}\\, log\\,( h_\\theta\\,(x^{(i)}))-(1-y^{(i)})\\,log\\,(1-h_\\theta(x^{(i)}))\\big]$</font>\n",
    "<p>\n",
    "Vectorized Cost Function\n",
    "<p>\n",
    "<font size=\"3em\">$J(\\theta) = \\frac{1}{m}\\big((\\,log\\,(g(X\\theta))^Ty+(\\,log\\,(1-g(X\\theta))^T(1-y)\\big)$</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def costFunction(theta, X, y):\n",
    "    m = float(y.size)\n",
    "    h = sigmoid(X.dot(theta))\n",
    "    \n",
    "    J = -1*(1/m)*(np.log(h).T.dot(y)+np.log(1-h).T.dot(1-y))\n",
    "               \n",
    "    if np.isnan(J[0]):\n",
    "        return(np.inf)\n",
    "    return(J[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "<a id='gradient_descent'></a>\n",
    "\n",
    "<p>\n",
    "The gradient of the cost is a vector of the same length as $θ$ where the $j^{th}$\n",
    "element (for $j = 0, 1, . . . , n$) is defined as follows:\n",
    "\n",
    "<p>\n",
    "<font size=\"4em\">\n",
    "$\\frac{\\delta J(\\theta)}{\\delta\\theta_{j}} = \\frac{1}{m}\\sum_{i=1}^{m} ( h_\\theta (x^{(i)})-y^{(i)})x^{(i)}_{j}$\n",
    "</font>\n",
    "\n",
    "<p>\n",
    "Vectorized\n",
    "\n",
    "<p>\n",
    "<font size=\"4em\">\n",
    "$\\frac{\\delta J(\\theta)}{\\delta\\theta_{j}} = \\frac{1}{m} X^T(g(X\\theta)-y)$\n",
    "</font>\n",
    "\n",
    "<p>\n",
    "Note that while this gradient looks identical to the linear regression gradient, \n",
    "<br>the formula is actually different because linear and logistic regression\n",
    "have different definitions of $h_θ(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gradient(theta, X, y):\n",
    "    \n",
    "    h = sigmoid(X.dot(theta.reshape(-1,1)))\n",
    "    \n",
    "    m = float(y.size)\n",
    "    grad =(1/m)*X.T.dot(h-y)\n",
    "    \n",
    "    return(grad.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial theta and cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "initial_theta = np.zeros(X.shape[1])\n",
    "cost = costFunction(initial_theta, X, y)\n",
    "grad = gradient(initial_theta, X, y)\n",
    "print('Cost: \\n', cost)\n",
    "print('Grad: \\n', grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing the cost function\n",
    "<a id='optimize_cost'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res = minimize(costFunction, \n",
    "               initial_theta, \n",
    "               args=(X,y), \n",
    "               method='Newton-CG', \n",
    "               jac=gradient, \n",
    "               options={'maxiter':400})\n",
    "\n",
    "theta = res.x\n",
    "\n",
    "# Print theta to screen\n",
    "print('Cost at theta found by minimize: ', res.fun)\n",
    "print('theta: ', theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "<a id='prediction'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(theta, X, threshold=0.5):\n",
    "    p = sigmoid(X.dot(theta.T)) >= threshold\n",
    "    return(p.astype('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = predict(theta, X) \n",
    "print('Train accuracy {}%'.format(100*sum(p == y.ravel())/p.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a student with an Exam 1 score of 45 and an Exam 2 score of 85, \n",
    "<br>you should expect to see an admission probability of 0.776 and prediction of 1 (admitted)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x1=np.array([1, 45, 85])\n",
    "print('admission probability: ', sigmoid(x1.T.dot(theta)))\n",
    "print('prediction: ', predict(theta, x1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Boundary\n",
    "<a id='decision_boundary'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(45, 85, s=60, c='r', marker='v', label='(45, 85)')\n",
    "plotData(data, 'Exam 1 score', 'Exam 2 score', 'Admitted', 'Not admitted')\n",
    "x1_min, x1_max = X[:,1].min(), X[:,1].max(),\n",
    "x2_min, x2_max = X[:,2].min(), X[:,2].max(),\n",
    "xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max), np.linspace(x2_min, x2_max))\n",
    "h = sigmoid(np.c_[np.ones((xx1.ravel().shape[0],1)), xx1.ravel(), xx2.ravel()].dot(res.x))\n",
    "h = h.reshape(xx1.shape)\n",
    "plt.contour(xx1, xx2, h, [0.5], linewidths=1, colors='b');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"bordo\">Regularized logistic regression</font>\n",
    "<p>\n",
    "In this part of the exercise, you will implement regularized logistic regression to predict whether microchips from a fabrication plant passes QA.\n",
    "<br>During QA, each microchip goes through various tests to ensure it is functioning correctly.\n",
    "\n",
    "<p>\n",
    "Suppose you are the product manager of the factory and you have the test results for some microchips on two different tests. \n",
    "<br>From these two tests, you would like to determine whether the microchips should be accepted or rejected. \n",
    "<br>To help you make the decision, you have a dataset of test results on past microchips, from which you can build a logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "<a id='regularized_load_data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data2 = loaddata('data/ex2data2.txt', ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = np.c_[data2[:,2]]\n",
    "X = data2[:,0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the data\n",
    "<a id='regularized_plot_data'></a>\n",
    "<p>\n",
    "Our dataset cannot be separated into positive and negative examples by a straight-line through the plot. \n",
    "<br>Therefore, a straight-forward application of logistic regression will not perform well on this dataset\n",
    "since logistic regression will only be able to find a linear decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plotData(data2, 'Microchip Test 1', 'Microchip Test 2', 'y = 1', 'y = 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature mapping\n",
    "\n",
    "<p>\n",
    "One way to fit the data better is to create more features from each data point.\n",
    "<br>We will map the features into all polynomial terms of x 1 and x 2 up to the sixth power.\n",
    "\n",
    "<p>\n",
    "<img src=\"images/mapFeature_x.png\" />\n",
    "\n",
    "<p>\n",
    "As a result of this mapping, our vector of two features (the scores on two QA tests) has been transformed into a 28-dimensional vector. \n",
    "<br>A logistic regression classifier trained on this higher-dimension feature vector will have\n",
    "a more complex decision boundary and will appear nonlinear when drawn in our 2-dimensional plot.\n",
    "\n",
    "<p>\n",
    "While the feature mapping allows us to build a more expressive classifier, \n",
    "it also more susceptible to <font color=\"red\">overfitting</font>. \n",
    "<br>In the next parts of the exercise, you will implement regularized logistic regression to fit the data and also see for yourself how regularization can help combat the overfitting problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mapFeatureVector(X1,X2):\n",
    "    \"\"\"\n",
    "    Feature mapping function to polynomial features. \n",
    "    Maps the two features X1, X2 to polynomial features. \n",
    "    X1, X2 must be the same size.\n",
    "    Returns -- New feature array with interactions and polynomial terms\n",
    "    \"\"\"\n",
    "    \n",
    "    degree = 6\n",
    "    output_feature_vec = np.ones(len(X1))[:,None] # raw vector of ones (sizeof X1)\n",
    "\n",
    "    for i in range(1,degree+1):\n",
    "        for j in range(i+1):\n",
    "            new_feature = np.array(X1**(i-j) * X2**j)[:,None]\n",
    "            output_feature_vec = np.hstack((output_feature_vec, new_feature))\n",
    "   \n",
    "    return output_feature_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized Cost Function\n",
    "<a id='regularized_cost_function'></a>\n",
    "<p>\n",
    "Recall that the regularized cost function in logistic regression is:\n",
    "<p>\n",
    "<font size=\"4em\">$J(\\theta) = \\frac{1}{m}\\sum_{i=1}^{m}\\big[-y^{(i)}\\, log\\,( h_\\theta\\,(x^{(i)}))-(1-y^{(i)})\\,log\\,(1-h_\\theta(x^{(i)}))\\big] + \\frac{\\lambda}{2m}\\sum_{j=1}^{n}\\theta_{j}^{2}$</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def costFunctionReg(theta, X, y, reg_param):\n",
    "    \"\"\"\n",
    "    Computes loss using sum of square errors for logistic regression\n",
    "    using theta as the parameter vector for linear regression to fit \n",
    "    the data points in X and y with penalty reg_param (lambda).\n",
    "    \"\"\"\n",
    "\n",
    "    m = float(y.size) # number of training examples\n",
    "    \n",
    "    h = sigmoid(np.dot(X, theta)) # hypothesis\n",
    "    \n",
    "    # Cost function J(theta)\n",
    "    reg_term = reg_param / (2*m) * np.sum(theta**2)\n",
    "    \n",
    "    J = (1/m) * np.sum(-y * np.log(h) - (1-y) * np.log(1-h)) + reg_term\n",
    "    \n",
    "    if np.isnan(J[0]):\n",
    "        return(np.inf)\n",
    "    return (J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized Gradient Descent\n",
    "<a id='regularized_gradient_descent'></a>\n",
    "<p>\n",
    "<u>Note</u>: you should not regularize the parameter $θ_0$.\n",
    "<p>\n",
    "For $j = 0$\n",
    "<p>\n",
    "<font size=\"4em\">$\\frac{\\delta J(\\theta)}{\\delta\\theta_{0}} = \\frac{1}{m}\\sum_{i=1}^{m} ( h_\\theta (x^{(i)})-y^{(i)})x^{(i)}_{j}$</font>\n",
    "<p>\n",
    "For $j >= 1$\n",
    "<p>\n",
    "<font size=\"4em\">$\\frac{\\delta J(\\theta)}{\\delta\\theta_{j}} = \\frac{1}{m}\\sum_{i=1}^{m} ( h_\\theta (x^{(i)})-y^{(i)})x^{(i)}_{j} + \\frac{\\lambda}{m}\\theta_{j}$</font>\n",
    "<p>\n",
    "Vectorized\n",
    "<p>\n",
    "<font size=\"4em\">$\\frac{\\delta J(\\theta)}{\\delta\\theta_{j}} = \\frac{1}{m} X^T(g(X\\theta)-y) + \\frac{\\lambda}{m}\\theta_{j}$</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradientReg(theta, X, y, reg_param):\n",
    "    \n",
    "    h = sigmoid(np.dot(X, theta)) # hypothesis\n",
    "    \n",
    "    # Vectorized implementation (replace with lines below)\n",
    "    #grad_reg = (1/m) * X.T.dot(h-y) + (reg_param / m) * np.r_[[[0]],theta[1:].reshape(-1,1)]\n",
    "    #return(grad.flatten())\n",
    "    \n",
    "    # Non-regularized\n",
    "    grad_0 = (1/m) * np.sum( (h-y)[:,None] * X, axis=0 ) # sum over the row axis\n",
    "    \n",
    "    # Regularized\n",
    "    grad_reg = grad_0 + (reg_param / m) * theta\n",
    "    \n",
    "    # Overwrite gradient for theta_0 with non-regularized gradient\n",
    "    grad_reg[0] = grad_0[0]\n",
    "    \n",
    "    return(grad_reg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
