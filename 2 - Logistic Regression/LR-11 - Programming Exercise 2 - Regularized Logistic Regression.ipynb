{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"bordo\">Programming Exercise 2 - Regularized Logistic Regression</font>\n",
    "\n",
    "<p>\n",
    "In this part of the exercise, you will implement regularized logistic regression to predict whether microchips from a fabrication plant passes QA.\n",
    "<br>During QA, each microchip goes through various tests to ensure it is functioning correctly.\n",
    "\n",
    "<p>\n",
    "Suppose you are the product manager of the factory and you have the test results for some microchips on two different tests. \n",
    "<br>From these two tests, you would like to determine whether the microchips should be accepted or rejected. \n",
    "<br>To help you make the decision, you have a dataset of test results on past microchips, from which you can build a logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "%matplotlib inline\n",
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loaddata(file, delimeter):\n",
    "    data = np.loadtxt(file, delimiter=delimeter)\n",
    "    print('Dimensions: ',data.shape)\n",
    "    print(data[0:6,:]) # print data samples\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotData(data, label_x, label_y, label_pos, label_neg, axes=None):\n",
    "    # Get indexes for class 0 and class 1\n",
    "    neg = data[:,2] == 0\n",
    "    pos = data[:,2] == 1\n",
    "    \n",
    "    # If no specific axes object has been passed, get the current axes.\n",
    "    if axes == None:\n",
    "        axes = plt.gca()\n",
    "    axes.scatter(data[pos][:,0], data[pos][:,1], marker='+', c='k', s=60, linewidth=2, label=label_pos)\n",
    "    axes.scatter(data[neg][:,0], data[neg][:,1], c='y', s=60, label=label_neg)\n",
    "    axes.set_xlabel(label_x)\n",
    "    axes.set_ylabel(label_y)\n",
    "    axes.legend(frameon= True, fancybox = True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "<a id='regularized_load_data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loaddata' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f71e288dea4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaddata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/ex2data2.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'loaddata' is not defined"
     ]
    }
   ],
   "source": [
    "data2 = loaddata('data/ex2data2.txt', ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = np.c_[data2[:,2]]\n",
    "X = data2[:,0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the data\n",
    "<a id='regularized_plot_data'></a>\n",
    "<p>\n",
    "Our dataset cannot be separated into positive and negative examples by a straight-line through the plot. \n",
    "<br>Therefore, a straight-forward application of logistic regression will not perform well on this dataset\n",
    "since logistic regression will only be able to find a linear decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plotData(data2, 'Microchip Test 1', 'Microchip Test 2', 'y = 1', 'y = 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature mapping\n",
    "\n",
    "<p>\n",
    "One way to fit the data better is to create more features from each data point.\n",
    "<br>We will map the features into all polynomial terms of x 1 and x 2 up to the sixth power.\n",
    "\n",
    "<p>\n",
    "<img src=\"images/mapFeature_x.png\" />\n",
    "\n",
    "<p>\n",
    "As a result of this mapping, our vector of two features (the scores on two QA tests) has been transformed into a 28-dimensional vector. \n",
    "<br>A logistic regression classifier trained on this higher-dimension feature vector will have\n",
    "a more complex decision boundary and will appear nonlinear when drawn in our 2-dimensional plot.\n",
    "\n",
    "<p>\n",
    "While the feature mapping allows us to build a more expressive classifier, \n",
    "it also more susceptible to <font color=\"red\">overfitting</font>. \n",
    "<br>In the next parts of the exercise, you will implement regularized logistic regression to fit the data and also see for yourself how regularization can help combat the overfitting problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mapFeatureVector(X1,X2):\n",
    "    \"\"\"\n",
    "    Feature mapping function to polynomial features. \n",
    "    Maps the two features X1, X2 to polynomial features. \n",
    "    X1, X2 must be the same size.\n",
    "    Returns -- New feature array with interactions and polynomial terms\n",
    "    \"\"\"\n",
    "    \n",
    "    degree = 6\n",
    "    output_feature_vec = np.ones(len(X1))[:,None] # raw vector of ones (sizeof X1)\n",
    "\n",
    "    for i in range(1,degree+1):\n",
    "        for j in range(i+1):\n",
    "            new_feature = np.array(X1**(i-j) * X2**j)[:,None]\n",
    "            output_feature_vec = np.hstack((output_feature_vec, new_feature))\n",
    "   \n",
    "    return output_feature_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized Cost Function\n",
    "<a id='regularized_cost_function'></a>\n",
    "<p>\n",
    "Recall that the regularized cost function in logistic regression is:\n",
    "<p>\n",
    "<font size=\"4em\">$J(\\theta) = \\frac{1}{m}\\sum_{i=1}^{m}\\big[-y^{(i)}\\, log\\,( h_\\theta\\,(x^{(i)}))-(1-y^{(i)})\\,log\\,(1-h_\\theta(x^{(i)}))\\big] + \\frac{\\lambda}{2m}\\sum_{j=1}^{n}\\theta_{j}^{2}$</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def costFunctionReg(theta, X, y, reg_param):\n",
    "    \"\"\"\n",
    "    Computes loss using sum of square errors for logistic regression\n",
    "    using theta as the parameter vector for linear regression to fit \n",
    "    the data points in X and y with penalty reg_param (lambda).\n",
    "    \"\"\"\n",
    "\n",
    "    m = float(y.size) # number of training examples\n",
    "    \n",
    "    h = sigmoid(np.dot(X, theta)) # hypothesis\n",
    "    \n",
    "    # Cost function J(theta)\n",
    "    reg_term = reg_param / (2*m) * np.sum(theta**2)\n",
    "    \n",
    "    J = (1/m) * np.sum(-y * np.log(h) - (1-y) * np.log(1-h)) + reg_term\n",
    "    \n",
    "    if np.isnan(J[0]):\n",
    "        return(np.inf)\n",
    "    return (J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized Gradient Descent\n",
    "<a id='regularized_gradient_descent'></a>\n",
    "<p>\n",
    "<u>Note</u>: you should not regularize the parameter $Î¸_0$.\n",
    "<p>\n",
    "For $j = 0$\n",
    "<p>\n",
    "<font size=\"4em\">$\\frac{\\delta J(\\theta)}{\\delta\\theta_{0}} = \\frac{1}{m}\\sum_{i=1}^{m} ( h_\\theta (x^{(i)})-y^{(i)})x^{(i)}_{j}$</font>\n",
    "<p>\n",
    "For $j >= 1$\n",
    "<p>\n",
    "<font size=\"4em\">$\\frac{\\delta J(\\theta)}{\\delta\\theta_{j}} = \\frac{1}{m}\\sum_{i=1}^{m} ( h_\\theta (x^{(i)})-y^{(i)})x^{(i)}_{j} + \\frac{\\lambda}{m}\\theta_{j}$</font>\n",
    "<p>\n",
    "Vectorized\n",
    "<p>\n",
    "<font size=\"4em\">$\\frac{\\delta J(\\theta)}{\\delta\\theta_{j}} = \\frac{1}{m} X^T(g(X\\theta)-y) + \\frac{\\lambda}{m}\\theta_{j}$</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradientReg(theta, X, y, reg_param):\n",
    "    \n",
    "    h = sigmoid(np.dot(X, theta)) # hypothesis\n",
    "    \n",
    "    # Vectorized implementation (replace with lines below)\n",
    "    #grad_reg = (1/m) * X.T.dot(h-y) + (reg_param / m) * np.r_[[[0]],theta[1:].reshape(-1,1)]\n",
    "    #return(grad.flatten())\n",
    "    \n",
    "    # Non-regularized\n",
    "    grad_0 = (1/m) * np.sum( (h-y)[:,None] * X, axis=0 ) # sum over the row axis\n",
    "    \n",
    "    # Regularized\n",
    "    grad_reg = grad_0 + (reg_param / m) * theta\n",
    "    \n",
    "    # Overwrite gradient for theta_0 with non-regularized gradient\n",
    "    grad_reg[0] = grad_0[0]\n",
    "    \n",
    "    return(grad_reg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
