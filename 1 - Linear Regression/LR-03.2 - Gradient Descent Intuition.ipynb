{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"bordo\">Gradient Descent Intuition</font>\n",
    "\n",
    "## Recap\n",
    "<img src=\"images/gd-partial-derivative.png\">\n",
    "\n",
    "## Assume we only have one parameter $θ_1$ $(θ_0 = 0)$\n",
    "\n",
    "- We minimize $J(θ_1)$ *(where $θ_1$ is a real number)*\n",
    "- Derivative says: \n",
    "  - Take the __*tangent*__ at a point and look at the <u>slope of the *tangent* line</u>\n",
    "  \n",
    "  <br>\n",
    "  \n",
    "  - Moving towards the minimum (<u>down</u>) we get a negative derivative: \n",
    "    <font size=\"4em\" color=\"blue\">$\\frac{d}{d(θ_1)} J(θ_1) < 0$</font>\n",
    "    - $α$ is always positive, so will update $θ_1$ to a bigger value \n",
    "      and $J(θ_1)$ will <u>decrease</u>: \n",
    "      <font size=\"4em\" color=\"blue\">$θ_1 = θ_1 - α \\cdot$(negative value)</font>\n",
    "    <img src=\"images/gradient descent  intuition - 2-1.png\">\n",
    "    \n",
    "  <br>\n",
    "  \n",
    "  - Similarly, Moving <u>up</u> a slope we get a positive derivative:\n",
    "    <font size=\"4em\" color=\"blue\">$\\frac{d}{d(θ_1)} J(θ_1) > 0$</font>\n",
    "    <br>$θ_1$ will be updated to a smaller value and $J(θ_1)$ will <u>also decrease</u>.\n",
    "    <img src=\"images/gradient descent  intuition - 2-2.png\">\n",
    "    \n",
    "  <br>\n",
    "  \n",
    "  - When you get to a local minimum\n",
    "    - Gradient of tangent (derivative) is $0$:\n",
    "      <font size=\"4em\" color=\"blue\">$\\frac{d}{d(θ_1)} J(θ_1) = 0$</font>\n",
    "    - So $θ_1 = θ_1 - 0$ and $θ_1$ remains the same *(convergence)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Learning Rate $α$\n",
    "- Alpha term $(α)$\n",
    "  - What happens if $α$ is too small or too large?\n",
    "    - Too small  ->  take baby steps (takes long time to converge)\n",
    "    - Too large  ->  can overshoot the minimum and fail to converge\n",
    "- As you approach the global minimum the derivative term gets smaller, \n",
    "  <br>so the update gets smaller, even when $α$ is fixed\n",
    "  - Meaning as the algorithm runs, we take smaller steps as we approach the minimum\n",
    "  - So no need to change $α$ over time\n",
    "<img src=\"images/gradient descent  intuition - 3.png\">\n",
    "\n",
    "## Question\n",
    "\n",
    "<img src=\"images/gradient descent  intuition - 4 - the learning rate alpha.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
