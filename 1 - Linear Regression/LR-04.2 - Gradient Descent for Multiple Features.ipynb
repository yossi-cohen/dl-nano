{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <font color=\"bordo\">Gradient descent for multiple variables</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis with multiple features\n",
    "- Parameters are $θ_0 .. θ_n$\n",
    "  <img src=\"images/Linear Regression Hypothesis - Multiple Features.png\" />\n",
    "<p>\n",
    "- For convenience of notation, define <font size=\"3em\">$x_0 = 1$</font>\n",
    "<p>\n",
    "$x=\\begin{bmatrix}\n",
    "1 \\\\\n",
    "x_{1} \\\\\n",
    "\\vdots \\\\\n",
    "x_{n} \\\\\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{n+1}$\n",
    "<p>\n",
    "- Think of the parameters as a single vector $θ$ of dimension $(n+1)$ \n",
    "<p>\n",
    "$θ=\\begin{bmatrix}\n",
    "θ_{0} \\\\\n",
    "θ_{1} \\\\\n",
    "\\vdots \\\\\n",
    "θ_{n} \\\\\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{n+1}$\n",
    "<p>\n",
    "- Our hypothesis for each sample $x^{(i)}$ looks like this:\n",
    "  <div align=\"center\"><font size=\"5em\">$h_θ(x{(i)})=θ^Tx^{(i)}$</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our cost function is\n",
    "<img src=\"images/gradient descent with multiple parameters - 1.png\">\n",
    "<br><div align=\"center\"><font size=\"4em\">$J(θ)=\\sum_{i=0}^{m}(θ^Tx^{(i)} - y^{(i)})^2$</font></div>\n",
    "\n",
    "### Full vectorized implementation\n",
    "- when we look at $X$ as a matrix in $\\mathbb{R}^{[m,n+1]}$ where each row is a sample, \n",
    "  <br> $θ \\in \\mathbb{R}^{n+1}$\n",
    "  <br>and vector of results $y \\in \\mathbb{R}^{m}$, one per sample, we get:\n",
    "<p>\n",
    "$X=\\begin{bmatrix}\n",
    "    1 & x_{11} & x_{12} & x_{13} & \\dots & x_{1n} \\\\\n",
    "    1 & x_{21} & x_{22} & x_{23} & \\dots & x_{2n} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    1 & x_{m1} & x_{m2} & x_{m3} & \\dots & x_{mn} \\\\\n",
    "\\end{bmatrix}$\n",
    "&nbsp;&nbsp;\n",
    "$θ=\\begin{bmatrix}\n",
    "θ_{0} \\\\\n",
    "θ_{1} \\\\\n",
    "\\vdots \\\\\n",
    "θ_{n} \\\\\n",
    "\\end{bmatrix}$\n",
    "&nbsp;&nbsp;\n",
    "$y=\\begin{bmatrix}\n",
    "y_{1} \\\\\n",
    "y_{2} \\\\\n",
    "\\vdots \\\\\n",
    "y_{m} \\\\\n",
    "\\end{bmatrix}$\n",
    "<p>\n",
    "- $J(θ)$ can be written as:\n",
    "<p>\n",
    "<div align=\"center\"><font size=\"4em\">$J(θ)=\\frac{1}{2m}\\sum(Xθ-y)^2$</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "<img src=\"images/gradient descent with multiple parameters - 2-2.png\">\n",
    "\n",
    "### Moving from 1 feature to n features will give us\n",
    "<img src=\"images/gradient descent with multiple parameters - 3-2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal Equation\n",
    "- An alternative (fast) way to get parameters without using gradient descent\n",
    "  <img src=\"images/linear equation.png\">\n",
    "- Use only for small or modarate data sets\n",
    "  - Computing the inverse doesn't scale (e.g., for 1024 x 768 pixel images)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
